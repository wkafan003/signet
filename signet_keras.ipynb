{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "signet_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqBQZMfEQ0be"
      },
      "source": [
        "Загрузка необходимых библиотек"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOu1XwiZ5-l8"
      },
      "source": [
        "try:\n",
        "    import gdown\n",
        "    import natsort\n",
        "except:\n",
        "    !pip install natsort gdown"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy3gE2wpA5Wl",
        "outputId": "71817fc3-41d3-4f69-f9c3-895fcc157645"
      },
      "source": [
        "try:\n",
        "    import ray\n",
        "    from ray import tune\n",
        "except:\n",
        "    !pip uninstall -y -q pyarrow\n",
        "    !pip install -q -U ray[all]\n",
        "    import os\n",
        "    # os._exit(0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 49.7MB 59kB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 54.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 52.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 52.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1MB 53.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 62.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 51.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 57.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 36.4MB 72kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 59.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 13.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 59.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 63.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 10.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25h  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: fastapi 0.63.0 has requirement starlette==0.13.6, but you'll have starlette 0.14.2 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfKBhhna5-mO"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import keras\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Activation, BatchNormalization, Input, Dropout, Flatten\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.layers import Lambda\n",
        "import natsort as ns\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVB3jg5cRJso"
      },
      "source": [
        "Пути к оригинальным подписям, фальшивым и папке с сохранеными весами модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2RMsMHnRJnf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L13V8vG_aAW9"
      },
      "source": [
        "PATH_ORG = \"signatures/full_org\"\n",
        "PATH_FORG =\"signatures/full_forg\"\n",
        "checkpoints_path = os.path.abspath(\"checkpoints/best/\")\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD5qB9A8V6Tk"
      },
      "source": [
        "Загрузка датасета Cedar, если он отсутствует "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoIJQTcD5-mS",
        "outputId": "84a1777f-4998-4ce6-e61c-870dac553d77"
      },
      "source": [
        "if os.path.exists('signatures.zip') is False:\n",
        "    !gdown https://drive.google.com/uc?id=1PpPVry5TkfGVpbFDkwOMNx7Xew4vscW5\n",
        "#"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PpPVry5TkfGVpbFDkwOMNx7Xew4vscW5\n",
            "To: /content/signatures.zip\n",
            "254MB [00:02, 125MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqGKFdSA5-mU"
      },
      "source": [
        "if os.path.exists('signatures') is False:\n",
        "    !unzip -q -n signatures.zip"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thnJlI1mWYi1"
      },
      "source": [
        "Класс, отвечающий за загрузку данных, lazy означает, что данные будут подгружаться по мере необходимости  - это медленее, чем загрузить все сразу, но не занимает места в памяти. Shuffle - перемешивать данные каждую эпоху."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBLZsJmOaai3"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, df, batch_size=32, dim=(155, 220), n_channels=3, shuffle=True, lazy=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.df = df\n",
        "        self.labels = df[\"label\"].to_numpy().astype(np.int32)\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.lazy = lazy\n",
        "        if self.lazy is False:\n",
        "            self.data = [np.empty((df.shape[0], *dim, n_channels), dtype=np.float32),\n",
        "                         np.empty((df.shape[0], *dim, n_channels), dtype=np.float32)]\n",
        "            for i in range(df.shape[0]):\n",
        "                image_1 = cv2.imread(df.iloc[i, 0])\n",
        "                image_1 = cv2.resize(image_1, (220, 155))\n",
        "                image_1 = 1-image_1/255.0\n",
        "\n",
        "                image_2 = cv2.imread(df.iloc[i, 1])\n",
        "                image_2 = cv2.resize(image_2, (220, 155))\n",
        "                image_2 = 1-image_2/255.0\n",
        "                self.data[0][i, :, :, :] = image_1\n",
        "                self.data[1][i, :, :, :] = image_2\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # return X, y\n",
        "        if self.lazy is False:\n",
        "            x = []\n",
        "            x.append(self.data[0][indexes, :, :, :])\n",
        "            x.append(self.data[1][indexes, :, :, :])\n",
        "\n",
        "            y = self.labels[indexes]\n",
        "\n",
        "        else:\n",
        "            rows = [self.df.iloc[k] for k in indexes]\n",
        "            x, y = self.__data_generation(rows)\n",
        "        return x, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(self.df.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, rows):\n",
        "        x_1 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        x_2 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        for i in range(len(rows)):\n",
        "            image_1 = cv2.imread(rows[i][\"image_1\"])\n",
        "            image_1 = cv2.resize(image_1, (220, 155))\n",
        "            image_1 = np.array(image_1)\n",
        "            image_2 = cv2.imread(rows[i][\"image_2\"])\n",
        "            image_2 = cv2.resize(image_2, (220, 155))\n",
        "            image_2 = np.array(image_2)\n",
        "            x_1[i,] = 1 - image_1 / 255.0\n",
        "            x_2[i,] = 1 - image_2 / 255.0\n",
        "            y[i] = rows[i][\"label\"]\n",
        "\n",
        "        return [x_1, x_2], y\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9wngdzWsC6"
      },
      "source": [
        "Евклидово расстояние (оно должно быть минмальным у оригиналов подписей и большим у оригинала и фальшивки)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akHC-9pD_QJb"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def euclidean_distance2(y):\n",
        "    return K.sqrt(K.sum(K.square(y[0] - y[1]), axis=-1))\n",
        "\n",
        "\n",
        "# def euclidean_distance(vects):\n",
        "#     x, y = vects\n",
        "#     sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "#     return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
        "\n",
        "\n",
        "# def eucl_dist_output_shape(shapes):\n",
        "#     shape1, shape2 = shapes\n",
        "#     return (shape1[0], 1)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv0gDn1pXE5C"
      },
      "source": [
        "Перефразируя Харшвардхана Гупту, мы должны помнить, что цель сиамской сети не в том, чтобы классифицировать набор пар изображений, а в том, чтобы различать их. По сути, контрастная потеря - это оценка того, насколько хорошо сиамская сеть различает пары изображений. Более математически обоснованные подробности о контрастных потерях, в статье\n",
        "http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-WsFbXsBKqY"
      },
      "source": [
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    sqaure_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    y_true = K.cast(y_true, y_pred.dtype)\n",
        "    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ns4rOkEa0pu"
      },
      "source": [
        "Данная функция создает сиамскую нейронную сверточную сеть  архитектуры signet.\n",
        "2 сиамские сети имеют одинаковые веса, после высчитывается евклидово расстояние от их выходов и передается функции потери. Несколько сверточный словек группируют графические признаки (такие как изгибы, завитки и т.д.) сначла в маленькие, потом средние и большие признаки. После 2 полносвязанных слоя являются классификатором, который по заданным признакам старается отличить подделку от оригинала. Слой dropout позволяет уменьшить эффект переобучения.\n",
        "\n",
        "В стандартной нейронной сети производная, полученная каждым параметром, сообщает ему, как он должен измениться, чтобы, учитывая деятельность остальных блоков, минимизировать функцию конечных потерь. Поэтому блоки могут меняться, исправляя при этом ошибки других блоков. Это может привести к чрезмерной совместной адаптации (co-adaptation), что, в свою очередь, приводит к переобучению, поскольку эти совместные адаптации невозможно обобщить на данные, не участвовавшие в обучении.\n",
        "\n",
        "Главная идея Dropout — вместо обучения одной DNN обучить ансамбль нескольких DNN, а затем усреднить полученные результаты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyBlQp235-mc"
      },
      "source": [
        "def make_net():\n",
        "    input = Input(shape=(155, 220, 3))\n",
        "\n",
        "    conv_1 = Conv2D(filters=96, kernel_size=(11, 11))(input)\n",
        "    batch_norm_1 = BatchNormalization()(conv_1)\n",
        "    activation_1 = Activation('relu')(batch_norm_1)\n",
        "    max_pool_1 = MaxPooling2D(pool_size=(3, 3))(activation_1)\n",
        "\n",
        "    conv_2 = Conv2D(filters=256, kernel_size=(5, 5))(max_pool_1)\n",
        "    batch_norm_2 = BatchNormalization()(conv_2)\n",
        "    activation_2 = Activation('relu')(batch_norm_2)\n",
        "    max_pool_2 = MaxPooling2D(pool_size=(3, 3))(activation_1)\n",
        "\n",
        "    dropout_1 = Dropout(rate=0.3)(max_pool_2)\n",
        "\n",
        "    conv_3_a = Conv2D(filters=384, kernel_size=(3, 3))(dropout_1)\n",
        "    activation_3_a = Activation('relu')(conv_3_a)\n",
        "    conv_3_b = Conv2D(filters=256, kernel_size=(3, 3))(activation_3_a)\n",
        "    activation_3_b = Activation('relu')(conv_3_b)\n",
        "    max_pool_3 = MaxPooling2D(pool_size=(3, 3))(activation_3_b)\n",
        "\n",
        "    # dropout_22 = Dropout(rate=0.3)(max_pool_3)\n",
        "    # conv_4_a = Conv2D(filters=384, kernel_size=(3, 3))(dropout_22)\n",
        "    # activation_4_a = Activation('relu')(conv_4_a)\n",
        "    # conv_4_b = Conv2D(filters=512, kernel_size=(3, 3))(activation_4_a)\n",
        "    # activation_4_b = Activation('relu')(conv_4_b)\n",
        "    # max_pool_4 = MaxPooling2D(pool_size=(2, 2))(activation_4_b)\n",
        "\n",
        "    dropout_2 = Dropout(rate=0.3)(max_pool_3)\n",
        "\n",
        "    flat_1 = Flatten()(dropout_2)\n",
        "    fc_1 = Dense(units=1024, activation='relu')(flat_1)\n",
        "    dropout_3 = Dropout(rate=0.5)(fc_1)\n",
        "    fc_2 = Dense(units=128, activation='relu')(dropout_3)\n",
        "\n",
        "\n",
        "\n",
        "    input_a = Input(shape=(155, 220, 3))\n",
        "    input_b = Input(shape=(155, 220, 3))\n",
        "\n",
        "    base_net = Model(input, fc_2)\n",
        "    processed_a = base_net(input_a)\n",
        "    processed_b = base_net(input_b)\n",
        "\n",
        "    distance = Lambda(euclidean_distance2)([processed_a, processed_b])\n",
        "    # distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "    model = Model([input_a, input_b], distance)\n",
        "    return base_net,model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZs0z08IcDDy"
      },
      "source": [
        "Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK_cT0L85-me"
      },
      "source": [
        "params = {\n",
        "    'dim': (155, 220),\n",
        "    'batch_size': 16,\n",
        "    # 'batch_size': 64,\n",
        "    'n_channels': 3,\n",
        "    'shuffle': False\n",
        "}\n",
        "\n",
        "\n",
        "def get_data(path_org, path_forg, test_size=0.3, random_state=0, lazy=True, ext_data=0):\n",
        "    org = ns.natsorted(os.listdir(path_org), alg=ns.IGNORECASE)\n",
        "    forg = ns.natsorted(os.listdir(path_forg), alg=ns.IGNORECASE)\n",
        "    org = [os.path.join(PATH_ORG, i) for i in org if i.endswith('.png')]\n",
        "    forg = [os.path.join(PATH_FORG, i) for i in forg if i.endswith('.png')]\n",
        "\n",
        "    org = [os.path.abspath(i) for i in org]\n",
        "    forg = [os.path.abspath(i) for i in forg]\n",
        "\n",
        "    samples = 24\n",
        "    ppl = len(org) // samples\n",
        "\n",
        "    data = []\n",
        "    for i in range(ppl):\n",
        "        tr = np.array([[org[j], org[j], 1] for j in range(i * samples, (i + 1) * samples)])\n",
        "        tr[:, 1] = np.concatenate([tr[:-12, 1], tr[-12:, 1]])\n",
        "        tr[:, 1] = np.random.permutation(tr[:, 1])\n",
        "        fl = np.array([[org[j], forg[j], 0] for j in range(i * samples, (i + 1) * samples)])\n",
        "\n",
        "        for j in range(ext_data):\n",
        "            rand2 = np.random.choice(\n",
        "                np.concatenate([np.arange(0, i * samples), np.arange((i + 1) * samples, ppl * samples)]),\n",
        "                samples, replace=False)\n",
        "            tr2 = np.array([[org[j], org[rand2[j % samples]], 0] for j in range(i * samples, (i + 1) * samples)])\n",
        "            data.append(tr2)\n",
        "\n",
        "        data.append(tr)\n",
        "        data.append(fl)\n",
        "\n",
        "    df = pd.DataFrame(np.array(data).reshape(-1, 3), columns=[\"image_1\", \"image_2\", \"label\"])\n",
        "    df = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "    ds_train, ds_val = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    train_datagen = DataGenerator(ds_train, **params, lazy=lazy)\n",
        "    validation_datagen = DataGenerator(ds_val, **params, lazy=lazy)\n",
        "    return train_datagen, validation_datagen\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOhDdNqncJ0Y"
      },
      "source": [
        "Подготовка датасета (так как хватает памяти, загрузим его \"жадно\", т.е. полностью в оперативную память\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnmcc7oJ8d3c"
      },
      "source": [
        "train_datagen, validation_datagen = get_data(PATH_ORG, PATH_FORG,\n",
        "                                             test_size=0.3, random_state=0, lazy=False, ext_data=0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pRFU3245-mn"
      },
      "source": [
        "x1 = (train_datagen.data[0], train_datagen.data[1])\n",
        "y1 = train_datagen.labels\n",
        "#\n",
        "x2 = (validation_datagen.data[0], validation_datagen.data[1])\n",
        "y2 = validation_datagen.labels\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PKZIXNzc2DL"
      },
      "source": [
        "Используем мощную библиотеку для подстройки гиперпараметров - ray tune. Библиотека позвоялет построить минимизировать время подбора параметров, умно отсекая бесперспективные варианты. Может работать как на кластере в несколько машин, но в нашем случае будет только серия последовательных экспериментов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFQ491_2dTjC"
      },
      "source": [
        "Далее опишем функцию обучения, которая будет сообщает ray о текущих результатах точности на тестовой и обучающейся выборке.\n",
        "В качестве метода оптимизации выберем моментный метод Адам, который благодаря накоплению импульса реже застревает в локальных экстремумах, а также увеличению влияние редко встречающихся объектов.\n",
        "https://habr.com/ru/post/318970/\n",
        "\n",
        "Авторы метода https://arxiv.org/abs/1412.6980 советуют принимать значения по-умолчанию как lr =0.001, b1=0.9,b2=0.999. Построим пространство поиска около этихзначений. А также инциализиуерм сид случайно (мультистарт), таким образом постараемся подобрать лучшие параметры оптимизатора. Также будем сохранять веса лучшей модели (для дальнейшего использования)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIxwJulh5-mv"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler, MedianStoppingRule, HyperBandScheduler\n",
        "from ray.tune.integration.keras import TuneReportCallback\n",
        "\n",
        "\n",
        "class TuneCheckpoint(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Tune Callback for Keras.\"\"\"\n",
        "\n",
        "    def __init__(self, metric='val_accuracy', logs=None):\n",
        "        self.best_metric=0\n",
        "        self.metric=metric\n",
        "        super(TuneCheckpoint, self).__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs[self.metric]>self.best_metric:\n",
        "            self.best_metric=logs[self.metric]\n",
        "            with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "                path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "                self.model.save(path)\n",
        "                # model.save(path)\n",
        "\n",
        "\n",
        "def train_signet(config,epochs, x1, x2, y1, y2,checkpoints_path):\n",
        "    # https://github.com/tensorflow/tensorflow/issues/32159\n",
        "    import tensorflow as tf\n",
        "\n",
        "    input_a = Input(shape=(155, 220, 3))\n",
        "    input_b = Input(shape=(155, 220, 3))\n",
        "\n",
        "    base_net,model = make_net()\n",
        "    optimizer = optimizers.Adam(learning_rate=config['lr'])\n",
        "\n",
        "    model.compile(loss=contrastive_loss, optimizer=optimizer, metrics=[accuracy])\n",
        "    # train_datagen,validation_datagen=get_data(PATH_ORG,PATH_FORG)\n",
        "\n",
        "    # model.fit(train_datagen, validation_data=validation_datagen,verbose=2, epochs=5,\n",
        "    #           callbacks=[TuneCheckpoint(),\n",
        "    #                TuneReportCallback({\n",
        "    #                    \"val_accuracy\": \"val_accuracy\",\n",
        "    #                    \"train_accuracy\": \"accuracy\",\n",
        "    #                })])\n",
        "    model.fit(x1, y1, batch_size=32, validation_data=(x2, y2),epochs=epochs, callbacks=[\n",
        "            # TuneCheckpoint(),\n",
        "            TuneReportCallback({\n",
        "                \"val_accuracy\": \"val_accuracy\",\n",
        "                \"train_accuracy\": \"accuracy\",\n",
        "            }),\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                filepath=checkpoints_path,\n",
        "                monitor='val_accuracy',\n",
        "                mode='max',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True)\n",
        "            ],\n",
        "            verbose=2)\n",
        "    \n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AQHL3dyemE5"
      },
      "source": [
        "Запустим tensoboard, обы в режиме реального времени отслеживать процесс оптимизации. Совет - включить автообновление в шестеренке и выбрать TIME SERIES и  HPARAMS пункты меню.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2xEjrV15-mx"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir  ~/ray_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEAKmb_Ue2Bp"
      },
      "source": [
        "В качестве планировщика выбран Hyperband. HyperBand для поиска наилучших конфигураций предлагает часто выполнять последовательное деление пополам с различными бюджетами.  epochs параметр - количество эпох в каждом испытании.  num_samples - количество испытаний (новых комбинаций гиперпарамтеров). Чем больше, тем лучше и тем дольше."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2MGYC_25-mx"
      },
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if ray.is_initialized() is False:\n",
        "        ray.init(num_cpus=4,num_gpus=1, local_mode=False)\n",
        "    sched = AsyncHyperBandScheduler(\n",
        "        time_attr=\"training_iteration\", max_t=400, grace_period=20)\n",
        "\n",
        "    analysis = tune.run(\n",
        "        tune.with_parameters(train_signet,epochs=12, x1=x1, x2=x2, y1=y1,\n",
        "                             y2=y2,checkpoints_path=checkpoints_path),\n",
        "        name=\"signet\",\n",
        "        scheduler=HyperBandScheduler(),\n",
        "        metric=\"val_accuracy\",\n",
        "        mode=\"max\",\n",
        "        stop={\n",
        "            \"val_accuracy\": 10,\n",
        "            \"training_iteration\": 15\n",
        "        },\n",
        "        num_samples=6,\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 4,\n",
        "            \"gpu\": 1\n",
        "        },\n",
        "        config={\n",
        "            \"lr\": tune.uniform(0.0005, 0.0015),\n",
        "            \"beta_1\": tune.uniform(0.89, 0.9),\n",
        "            \"beta_1\": tune.uniform(0.998, 0.9995),\n",
        "  \n",
        "        },\n",
        "        keep_checkpoints_num=1,\n",
        "        checkpoint_score_attr=\"val_accuracy\",\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTdnnUsbfRIq"
      },
      "source": [
        "Лучший набор гиперпарметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxhApadO5-m1"
      },
      "source": [
        "print(\"Best hyperparameters found were: \", analysis.best_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIKWQLvPfUuI"
      },
      "source": [
        "Лучший результат"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCv59QFH5-m2"
      },
      "source": [
        "analysis.best_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saHLBjPrYREE"
      },
      "source": [
        "Загрузим веса модели для проверки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOnrXvNM5-m9"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "_,loaded_model = make_net()\n",
        "loaded_model.load_weights(checkpoints_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Belka0S65-nA"
      },
      "source": [
        "org = ns.natsorted(os.listdir(PATH_ORG), alg=ns.IGNORECASE)\n",
        "forg = ns.natsorted(os.listdir(PATH_FORG), alg=ns.IGNORECASE)\n",
        "org = [os.path.join(PATH_ORG, i) for i in org if i.endswith('.png')]\n",
        "forg = [os.path.join(PATH_FORG, i) for i in forg if i.endswith('.png')]\n",
        "\n",
        "org = [os.path.abspath(i) for i in org]\n",
        "forg = [os.path.abspath(i) for i in forg]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deq95RixfnxP"
      },
      "source": [
        "Должно быть большое значение - подпись и фальшивка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thmhyuz45-nB"
      },
      "source": [
        "im_1 = cv2.imread(org[0])\n",
        "im_2 = cv2.imread(forg[5])\n",
        "im_1 = cv2.resize(im_1,(220,155))\n",
        "im_2 = cv2.resize(im_2,(220,155))\n",
        "im_1 =1- im_1/255.0\n",
        "im_2 = 1-im_2/255.0\n",
        "im_1 = np.expand_dims(im_1,axis=0)\n",
        "im_2 = np.expand_dims(im_2,axis=0)\n",
        "y_pred = loaded_model.predict([im_1,im_2])\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IQOFUWufrXa"
      },
      "source": [
        "Сохраним веса в гугл диск"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiJ-EeplXYr-"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q30iA4kvkach"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDHbDXEMkaSp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5cra-rOkZ4J"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GSM5SNPkZq0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuZRSF8Om8sb"
      },
      "source": [
        "!rm -rf '/content/drive/MyDrive/Colab Notebooks/Signet/best_model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV51qejoUOCu"
      },
      "source": [
        "!cp -R '/content/checkpoints' '/content/drive/MyDrive/Colab Notebooks/Signet/best_model'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}